特征工程
	需要哪些数据
		一般公司内部数据，用户行为日志，业务数据，第三方数据
	数据如何存储
	数据清洗
		数据清洗占开发过程的30-50%
		预处理
			选择数据处理工具，数据库和python
			查看数据的元数据以及数据特征，一查看元数据，包括字段解释、数据来源等一起可以描述数据的信息；另外抽取一部分数据，通过人工查看的方式，对数据本身做一个比较直观的了解，并初步发生一些问题，为之后的数据处理做准备。	
		清洗异常样本数据
			格式内容错误数据清洗
				一般情况存在格式和内容上下不一致的情况，所以建模前进行内容清洗操作
					时间、日期、数值、半全角等线上格式不一致
					内容有不该存在的字符：典型的就是在头部、中间、尾部的空格问题，采用半自动校验加半人工方式来找问题，并去除不需要的字符
					内容与该字段应有的内容不符：比如姓名写成了性别，身份证号写成了手机号等
				逻辑错误信息，通过简单逻辑推理发现数据中的问题数据，防止分析结果走偏：
					数据去重
					去除|替换不合理的值
					去除|重构不可靠的字段值（修改矛盾的内容）
				去除不需要的数据
					数据收集尽可能多的字段，但字段多不一定效果好，需要进行字段的删除
				关联性验证
					如果数据有多个来源，有必要进行关联验证。
		采样
			数据不平衡，实际应用中，数据分布非常不均匀，会出现长尾现象，即绝大多数的数据在一个范围|属于一个类别，二在另外一个范围或者另外一个类别，只有很少的数据。
				设置损失函数的权重
					使用少数类别数据判断错误的损失大于多数类别数据判断错误的损失。可通过skikit-learn中的class_weight参数来设置权重
				下采样|欠采样
					从多数类中随机抽取样本从而减少多数类别样本数据，使数据达到平衡的方式
				集成下采样|欠采样
					采用普通的下采样会导致信息丢失，所以一般采用集成学习和下采样结合的方式解决问题。
						EasyEnsemble 采用不放回的数据抽取方式抽取多数类别样本数据，然后抽取出来的数据和少数类别数据组合训练一个模型；多次这样操作，构建多个模型，然后使用多个模型共同决策|预测4
						BalanceCascade 利用Boosting增量思想来训练模型；先通过下采样产生训练集，然后使用Adaboost算法训练一个分类器；然后使用该分类器对所有大众样本数据进行预测，
										并将预测正确的样本从大众样本数据中删除；重复迭代上述两个操作，直到大众样本数据量等于小众样本数量。
						Edited Nearest Neighbor(ENN):对于多数类别样本数据而言，如果这个样本的大部分k近邻样本都和自身类型不一样，那就将其删除，然后使用删除后的数据进行训练
						Repeated Edited Nearest Neighbor(RENN)对于多数类别样本数据而言，如果这个样本的大部分k近邻样本都和自身类型不一样，那就将其删除,重复以上步骤，直到数据集无法再被删除。
						Tomek Link Removal：如果两个不同类别的样本，他们的最近邻都是对方。也就A的最近邻是B，B的最近邻是A，那么AB就是Tomek Link。将所有Tomek Link中多数类别的样本删除。
				过采样|上采样
						通过有放回抽样，不断的从少数类别样本数据中抽取样本，然后使用抽取样本+原始数据组成训练数据集进行训练。不过该放手比较容易导致过拟合，一般抽样样本不要超过50% 。可以随机抽取的样本对数据的各个维度可以进行随机的小范围变动，eg：(1,2,3)-->(1.01,1.99,3),采用该放手可以相对降低上采样导致的过拟合问题
					    采用数据合成的方式生成更多的样本，该放手在小数据集场景下具有比较成功的案例。常见算法SMOTE，该算法利用小众样本在特征空间的相似性来生成新样本。
						看成一分类问题（One class learning）或者异常检测问题，对于其中一个类别进行建模，然后对所有不属于这个类别的数据认为是异常数据，经典算法包括：one Class SVM、IsolationForest
				Focal Loss
特征转换
	文本数据转换成数值数据
	缺省值填充
	定性特征属性哑编码
	定量特征属性二值化
	特征标准化与归一化
分词
	按照文本|单词特征进行划分
	词典匹配：匹配方式可以从左到右，从右到左。对于匹配中遇到的多种分段可能性，通常会选取分隔出来词的数据最小的。
	基于统计的方式：隐马尔可夫（HMM）、最大熵模型（ME），估计相邻汉子之间的关联性，进而实现切分
	基于深度学习：神经网络抽取特征、联合建模
jieba分词原理
	字符串匹配：把汉子串与词典中的词条进行匹配，识别出一个词
	理解分词法：通过分词子系统、句法语义子系统、总控部分来模拟人堆句子的理解（试验阶段）
	统计分词法：简历大规模语料库，通过HMM或其他模型，进行分词（主流方法）

缺省值填充
	确定缺失值范围
	去除不需要的字段
	填充缺省内容
	重新获取数据
	注意：最重要的是缺失值内容填充
		以业务知识或经验推测填充缺省值
		以同一字段指标的计算结果（均值、中位数、众数等）填充缺省值
		以不同字段指标的计算结果来推测行的填充缺省值。
		sklearn主要通过Imputer类来实现缺省值填充
哑编码（OneHotEncoder）
二值化（Binarizer），对于定量的数据（特征值连续）根据给定的阀值，将其进行转换，如果大于阀值，那么为1，否则为0
标准化(z-score)：基于特征属性的数据（也就是特征矩阵的列），获取均值和方差，然后将特征值转换至服从标准正太分布。
		StandardScaler
区间缩放法
	x=x-x.min  / x.max-m.min
	MinMaxScaler
正则化，和标准化不同，正则化是基于矩阵的行进行数据处理，目的将行转换为单位向量，l2规则如下
	Normalizer
	
数据多项式扩展变换，PolynomialFeatures

GBDT | RF+LR

特征选择
	通常从两方面来选择特征：
		特征是否发散
		特征与目标的相关性
	特征选择方法以下三种：
		Filter 过滤法，方差选择法、相关系数法、卡方检验、互信息法
			方差选择法：VarianceThreshold计算各个特征属性的方差值，然后根据阀值，获取方差大于阀值的特征
			相关系数法：SelectKBest 先计算各个特征属性对于目标值得相关系数以及阀值k，然后获取k个相关系数最大的特征属性
			卡方核验：检测定性自变量对定性因变量的相关性
			递归特征消除法：RFE使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，在基于新的特征集进行下一步训练
		Wrapper 包装法
		
		Embedded 嵌入法
			基于惩罚项的特征选择法：SelectFromModel，使用惩罚项的基模型，除了可以筛选出特征外，同时还可以进行降维操作
降维，常见降维方法除了基于L1的惩罚模型外，还有PCA和LDA。PCA是为了映射后样本具有最大的发散性，LDA是为了映射后的样本有最好的分类性能。
	降维的目的减少特征属性的个数，确保特征属性之间的相互独立性
		主成分分析法PCA（无监督）sklearn PCA
		线性判别分析法LDA（有监督）sklearn LinearDiscriminantAnalysis
			将带标签的数据点，通过投影的方法，投影到维度更低的空间中，使的投影后的点，会形成按类别区分，一簇一簇的情况，相同的类别的点会在投影后的空间中更接近。用一句话：投影后类内方差最小，类间方差最大
		PCA与LDA对比
			相同点：
				均可以对数据完成降维操作
				均使用矩阵分解的思想
				都假设数据符合高斯分布
			不同点：
				LDA有监督，PCA无监督
				LDA降维最多降到类别数据k-1的维数，而PCA没有限制
				LDA除了降维外，还可以应用于分类
				LDA选择的分类性能最好的投影，而PCA选择样本点投影具有最大方差的方向
