聚类算法，无监督算法，训练数据只有特征属性x，没有类别y，模型通过找x的特征新，将数据划分为不同的类别，基于这样的划分，用于样本数据x任务的和那个类别最接近来产生预测
	常用聚类算法：KMeans，GMM高斯混合聚类，LDA

	闵可夫斯基局里
		p=1是曼哈顿距离
		p=2是欧氏距离（用的多）
		p无穷大是切比雪夫距离
	相似度
		余弦夹角相似度
		KL距离
		杰卡德相似系数（适合稀疏）
		Pearson相关系数
	聚类思想，给定一个有M个对象的数据集，构建一个具有k个簇的模型，欺诈k<=M
		满足以下条件：
		每个簇至少包含一个对象
		每个对象属于且属于一个簇（绝大多数)
		将满足上述条件的k个簇成为一个合理的聚类划分
	基本思想，对于给定的类别数目k，首先给定的初始划分，通过迭代改变样本和簇的隶属关系，使的每次处理后得到的划分方式比上一次的好（总数据集之间的距离和变小了）
K-means算法，
	在迭代过程中使用所有点的均值作为新的质点（中心），如果簇中存在异常点，将导致均值偏差比较严重，此时使用中位数比较好，使用
	使用中位数的聚类方式叫K-Mediods（K中值聚类）
	K-mans是初值敏感的，选择不同的初始值可能导致不同的簇划分，使用多次
	
	缺点：
		K值是用户给定的，在数据处理前，K值是未知的，不同的k值得到不同的结果
		对初始簇中心点敏感
		不适合发现非凸形状的簇或者大小差别较大的簇
		特殊值（离群值）对模型的影响比较大
	优点：
		理解容易，聚类效果不错
		处理大数据集的时候，该算法可以保证较好的伸缩性和高效率
		当簇近似高斯分布的时候，效果非常不错
二分K-means算法，解决对初始簇心比较敏感的问题
	是一种弱化初始质心的一种算法，具体步骤：
		将所有样本作为一个簇放到一个队列中
		从队列中选择一个簇进行kmeans算法划分，划分为两个子簇，并将子簇添加到队列中
		循环迭代第二步操作，直到中止条件达到（【聚簇数量】、最小平方误差、迭代次数）
		队列中的簇就是最终的分类簇集合
	队列中选择划分簇的规则一般两种方式：
		对所有簇计算误差和SSE（距离函数的一种变种），选择SSE最大的聚簇进行划分操作（优选这种策略）
		选择样本数量最多的簇进行划分操作
K-means++，对初始簇心比较敏感的问题
	与k-means的主要区别在于初始的k个中心点的选择方面		
		k-means是随机给定的方式
		k-means++以下步骤：
			从数据集中任选一个节点作为第一个簇类中心
			对数据集中的每个点x，计算x到所有已有聚类中心点的距离和D（X），基于D（X）采用线性概率选择出下一个聚类中心点（距离较远的一个点成为新增加的簇中心）
			重复步骤2直到找到k个聚类中心点
	缺点：由于聚类中心点的选择过程中的内在有序性，在拓展方面存在性能方面的问题（第k个聚类中心点的选择依赖前k-1个聚类中心点的值）
K-Means，解决K-means++算法去电二产生的一种算法
	主要思路每次便利时，取样规则不是每次只获取一个样本，而是每次获取K个样本，重复O词，然后再讲这些抽样出来的聚类出k个点，最后使用k个点作为k-Means算法的初始聚簇中心点
	实践证明，一般5次重复采用就可以保证一个比较好的聚簇中心点

----------------------
Canopy算法，是“粗”聚类算法，执行速度快，但精度低
	K-Means算法存在初始聚簇中心点敏感的问题，常用Canopy+K-Means算法混合形式进行模型构建
		先使用cannopy算法进行粗聚类得到k个聚类中心点
		K-Means算法使用Canopy算法得到的k个聚类中心点作为初始中心点，进行细聚类
	优点：
		执行速度快
		不需要给定k值，应用场景多
		能缓解K-Means算法对于初始聚类中心点敏感的问题
-----------------------
Mini Batch K-Means算法 
	一种优化变种，采用小规模的数据子集，减少计算时间，同时试图优化目标函数。Mini Batch- K-Means算法可以减少K-Means算法的收敛时间，而产生的效果略茶语标准K-Means算法
	
	聚类算法的衡量指标
		均一性，完整性，完整性，V-measure，Rand index（兰德指数），调整互信息AMI
		
		簇内不相似度，簇间不相似度，轮廓系数
K-Means和Mini Batch K-Means算法效果评估
