决策树
	条件熵H(Y|X)=H(X,Y)-H(X),给定条件X的情况下，所有不同X值情况下Y的信息熵的平均值叫做条件熵
	分为分类数和回归树，前者用于分类标签值，后者用于预测连续值
	求信息熵增溢=信息熵-条件熵，寻找划分节点，信息熵不变，条件熵越小，熵增溢越大。
	1.ID3
		1.是经典的构造算法，内部使用信息熵以及信息增益来进行构建，每次迭代选择信息增益最大的特征属性作为分割属性
		2.Gain(A)=H（D）-H（D|A）
		3.只支持离散的特征属性，不支持连续的特征属性
		4.构建的是多叉树
		5.优点：构建速度快，实现简单
		6.缺点：计算依赖特征数目较多的特征，而属性值最多的属性并不一定最优（分支越多，相对增益熵越大）
				不是递增算法
				单变量决策树，对特征属性之间的关系不会考虑
				抗噪性查
				只适用于小规模数据集，需要将数据放到内存中
	
	2.C4.5（c5算法是商业算法，收费的）
		1.使用信息增益率来取代ID3算法中的信息增益，在书的构造过程中进行剪枝的优化，能自动完成对连续属性的离散处理
		2.Gain(A)=H（D）-H（D|A），Gain_ratia(A)=Gain(A) / H(A)
		3.优点：产生的规则易于理解，准确率比ID3较高，实现简单
		4.缺点：对数据集需要进行多次顺序扫描和排序，所以效率低（比ID3慢一点）
				只适合小规模数据集，需要将数据放到内存中

	3.CART
		1.使用基尼系数(分类树)作为数据纯度的量化指标构建的决策树
			Gini=1-sum_i_n P(i)**2
		2.使用GINI增益作为分割属性的选择的标准，选择GINI增益最大的作为当前数据集分隔属性
			Gain(A）=Gini(D）-Gini（D|A）)
 		4.CART构建的是二叉树
	4.对比
		ID3，C4.5只适合在小规模数据集使用
		ID3，C4.5都是单变量决策树
		当属性取值比较多的时候，最好考虑C4.5，ID3的出的效果比较差
		决策树分类一般情况只适合小数据量的情况（数据可以放内存）
		CART是三种算法中最常见的一种决策树构建算法（sklearn中仅支持CART）
		三种算法的区别仅仅只是对于当前数的评价标准不同而已，ID3使用信息增益，C4.5使用信息增益率，CART使用基尼系数
		CART算法构建的一定是二叉树，ID3和C4.5构建的不一定是二叉树
		
		算法	支持模型	树结构	特征选择	连续值处理	缺失值处理	剪枝	特征属性多次使用
		ID3		分类		多叉树	信息增益	不支持		不支持		不支持	不支持
		C4.5	分类		多叉树	信息增益率	支持		支持		支持	支持
		CART	分类		二叉树	基尼系数	支持		支持		支持	支持
				回归				均方差
										
	5.决策树优化策略
		5.1 剪枝优化
			决策树过度拟合一般情况是由于节点太多导致的，剪枝优化对正确率影响比较大的，是一种常用优化方式：
				a. 前置剪枝,构建决策树的过程中，提前停止，结果决策树一般比较小，实践证明这个策略无法取得比较好的结果
				b. 后置剪枝，决策树构建好后，然后再开设裁剪，一般两种方式：
						用单一叶子节点代替整个子树，叶节点的分类采用子树中最主要的分类
						将一个子树完全替代另一个课子树
						后置剪枝的主要问题是计算效率问题，存在一定的浪费情况。
					后置剪枝过程：
						有完全数T0开始，剪枝部分节点得到T1，在此剪枝得到T2，直到仅剩树根的数Tk
						在验证数据集熵对着k+1个数进行评价，选择最优的数Ta
						对于给定的T0：
								计算所有内部非叶子节点的剪枝系数
								查找最小的剪枝系数节点，将其删除。存在对个最小剪枝系数几点，选择包含数据项最多的几点删除
								重复上述操作，知道产生剪枝决策树只有1个节点
								得到决策树T0T1T2...Tk
								使用验证演变集选择最优子树Ta
		5.2 Random Forest 随机森林
			利用训练数据随机产生多个决策树，形成一个森林，然后使用盛林进行预测，选取最多结果作为预测结果
	6.分类数和回归树的区别
		6.1分类树采用信息增益，信息增益率，基尼系数来评价树的效果，都是基于概率值进行判断的；二分类树的叶子节点的预测值
					一般为叶子节点中概率最大的类别作为当前叶子的预测值
		6.2在回归树中，叶子节点的预测值一般为叶子节点中所有值得均值来作为当前叶子节点的预测值。所以回归树中一般采用MSE作为树的评价指标，即均方差
			一般情况下，只使用CART算法构建回归树
	7.决策树可视化
		pipinstall install graphviz pydotplus
		读取将决策树的dot模型，显示各节点信息。