 	集成学习思想是将若干个学习器（分类器&回归器件）组合之后产生一个新的学习器。集成算法成功在于保证弱分类器的多样性。而且集成不稳定的算法也能够得到一个比较明显的性能提升
	为什么需要集成学习：弱分类器存在一定的查一下，个别会存在错误，整体会减少错误率
						数据集过大或过小，可以分别进行划分和有放回的操作产生不同数据子集，使用子集训练不同的分类器，最终合并成为一个大的分类器
						如果数据划分边界过于复杂，使用线性模型很难描述，可以训练多个模型，然后再进行模型的融合
						对于多个异构的特征集的时候，很难进行融合，那么可以考虑每个数据集构建异构分类模型，然后将多个模型融合
	集成休息思想：Bagging、Boosting、Stacking
二、Bagging自举汇聚法 
	在原始数据集上通过有放回的抽样方式，重新选择出S个新的数据集来分别训练S个分类器的集成技术
	在预测新样板的分类的时候，会使用多数投票或者求均值的方式统计最终分类结果
	弱学习器可以是基本的算法模型：Linear、Ridge、Lasso、Logistic、Softmax、ID3、C4.5、CART、SVM、KNN等
	有放回抽样，去掉重复数据后，训练模型的数据集样本（66.7%）与原始样本（100%）的是不一致的。
	1.随机森林RF（Random Forest）在Bagging策略基础上进行修改后的一种算法
		从原始样本集（n个样本）中用Bootstrap采样（有放回重采样）选出n个样本，去重后实际参与训练数据集小于n
		从所有属性中随机选择K个属性，选择出最佳分割属性作为节点创建决策树
		重复以上两步m次，即建立m颗决策树
		这m个决策树形成随机盛林，通过投票表决结果决定数据属于哪一类
	2.RF实际应用中有比较好的特性，应用较广泛，主要应用在：分类、回归、特性转换、异常点检测等
		RF变种：ExtraTree、Totally Random Trees Embedding（TRTE）、Isolation Forest）在Bagging策略基础上进行修改后的一种算法
		Extra Tree，原理基本和RF一样，区别如下：
				RF会随机重采样训练集，ExtraTree使用原始数据集训练
				RF在选择划分特征点的时候和传统决策树一样，会基于信息增益、信息增益率、基尼系数、均方差等原则来选择最优特征值，ExtraTree会随机选择一个特征值来划分决策树。
				Extra Tree因为随机选择特征值划分点，会导致决策树规模大于RF生成的决策树。方差相对RF进一步减少，在某些情况下泛化能力比RF强。	
		TRTE，是一种非监督的数据转化方式，将低维的数据映射到高维，从而让映射到高维的数据更好的应用于分类回归默写
				建立T个决策树来拟合数据（类似于KD-TREE一样基于特征属性的方差选择划分特征）。当决策树构建完成后，数据集的每个数据在T个决策树中叶子节点的位置就定下来了，将位置信息转换为向量就完成了特征转换操作。
		IForest，是一种异常点检测算法，使用类似RF的方式来检测异常点
				随机采样过程中，一般只需要少量数据即可
				在进行决策树构建过程中，IForest会随机选择一个划分特征，并对划分特征随机选择一个划分阀值
				IForest构建的决策树一般深度max_depth是比较小的
				区别原因是目的是异常点检测，所以只需要能够区分异常即可，不需要大量数据。一般不需要太大的规模的决策树
三、Boosting提升学习
	可用于回归和分类问题，每一步产生弱预测模型，并加权累加到总模型中；如每一步弱预测模型的生成都是依据损失函数的梯度方式的，称为梯度提升（Gradient boosting）
	提升的技术意义：如果一个问题存在弱预测模型，那么可以通过提升技术的方法得到一个强预测模型
	常见的模型有：Adaboost、Gradient Boosting（GBT/GBDT/GBRT）
	Adaptive Boosting,一种迭代算法，每轮迭代会在训练集上产生一个新的学习器，根据学习器对所有样本进行预测，以评估每个样本的重要性。对于预测正确的权重低，预测错误的权重高，用于新的学习器训练
		Adaboost将基分类器的线性组合作为强分类器，给分类误差率小的基本分类器以大的权值，给分类误差率较大的基本分类器以小的权值
		f(x)=sum_i_M am*Gm(X)
		最终分类器在线性组合上Sign函数转换，G(x)=sign(f(x))
		损失函数（以错误率为损失函数） loss=1/n * sum_i_n I(G(xi) != yi) 
		优点：可以处理连续值和离散值，鲁棒性强，解释强，结构简单
		缺点：对异常样本敏感，异常样本可能算法会在迭代过程中获得较高的权 重值，最终影响模型效果
	GBDT梯度提升迭代决策树，和Adaboosting区别：Adaboost利用前一轮弱学习器的误差来更新样本权重值，然后一轮一轮迭代；GBDT也是迭代，但要求弱学习器必须是CART模型，而且在模型训练的时候，要求模型预测的样本损失尽可能的小。
		所有GBDT算法中，低层都是回归树
		GBDT由三部分组成：DT（REgression Decision Tree）、GB（Gradient Boosting）、Shrinkage（衰减）
		有多颗决策树组成，所有树的结果累加起来就是最终结果
		迭代决策树和随机森林的区别：
			随机森林使用抽取的样本构建不同的子树，第m颗树和前m-1颗树的结果是没有关系的
			迭代决策树，使用之前树构建结果后形成的残差作为输入数据构建下一个子树，最终预测的时候，安装子树构建顺序进行预测，并将预测结果相加
		优点：可处理连续值和离散值；在相对少的调参情况下，模型预测效果也会不错；鲁棒性比较强
		缺点：弱学习器之间存在关联关系，难以并行训练模型		
四、Bagging、Boosting的区别
	样本选择：Bagging是有放回随机采样，Boosting每一轮训练集不变，至少训练中分类器中的样本权重或目标属性y值发生变化
	样本权重：Bagging样例权重一样，Boosting根据错误率不断调整样例的权重值，错误率越大权重越大
	预测函数：Bagging所有预测模型的权重相等，Boosting对误差小的分离器具有更大的权重
	并行计算：Bagging可并行运算，Boosting不能并行只能顺序生成
	Bagging是减少模型的variance（方差），Boosting是减少模型的Bias（偏度）
	Bagging里每个分类模型都是强分类器，因为降低的是方差，方差过高需要降低是过拟合；Bagging里每个分类模型都是弱分类器，因为降低的是偏度，偏度过高时欠拟合。
五、Stacking指训练一个模型用于组合其他模型（基模型、基学习器）的技术。（神经网络中其实就是类似的）
		即首先训练出多个不同的模型，然后再以之前训练的各个模型的输出作为输入来新训练一个新的模型，从而得到一个最终的模型。一般情况下使用单层的logistic回归作为组合模型。
六、XGBoost（开源的支持多语言）是GBDT算法的一种变种，是一个常用的有监督集成学习算法，是一种伸缩性强、便捷的可并行构建模型的Gradient Boosting算法。
	XGBoost和GBDT比较
		1.XGBoost在GBDT的基础上加入了正则化项，防止模型过拟合
		2.XGBoost在构建的过程中考虑的二阶导函数，GBDT只考虑一阶导函数
		3.XGBoost中的决策树的构建是基于损失函数，GBDT的决策树CART是基于MSE、MAE、Gini。。。
		4.XGBoost支持列采样（类似随机森林的方式），可以降低过拟合的情况
		5.XGBoost是并行计算指的是划分特征选择过程中是并行计算的
		6.XGBoost低层支持CART、线性回归、逻辑回归等多种算法，GBDT只支持CART	
		不做模型调参时优先使用GBDT，数据量太大训练速度慢时考虑XGBoost（需要调参）
	树节点分裂方法
		精准算法：遍历所有特征的所有可能的分割点，计算gain值，选择最大的gain值对于的（feature，value）进行分割
		近似算法：对于每个特征，只考虑分位点，减少计算复杂度
		XGBoost树节点划分方法，不是简单的安装样本的个数进行分位的，而是按照上一轮的预测误差函数的二阶导数值作为权重来进行划分的
	XGBoost其他特性：
		列采样，借鉴随
		
	
	
	