SVM，本身是一个二元分类算法，是对感知器算法模型的一种扩展，现在的SVM算法支持线性分类和非线性分类，并且也能够直接将SVM应用于回归应用中，同时通过OvR或者OvO的方式，应用于多元分类领域中。在不考虑集成学习算法，不考虑特定的数据集的时候，在分类算法中SVM可以说是特别优秀的。

线性可分SVM，让离超平面比较进的点尽可能的远离这个超平面，那么我们的模型分类效果比较好
		要求数据必须是线性可分的
		纯线性可分的SVM模型对于异常数据的预测可能会不太准
		对于线性可分的数据，SVM分类器的效果非常不错
	SVM的软间隔模型，由异常数据点导致线性不可分，可以用软间隔解决。
			硬间隔：
				min 1/2 * ||w||**2
				st：yi（wxi+b）>= 1
			软间隔
				min 1/2 * ||w||**2 + C*sumei
				st：yi（wxi+b）>= 1-ei
		软间隔可以解决线性数据中携带异常点的分类模型构建的问题
			  可以引入惩罚项系数，可以增加模型的泛化能力，即鲁棒性
			  如果惩罚系数小，表示模型构建时，允许存在越多的分类错误样本，模型准确率比较低；如果惩罚系数大，表示模型构建时，越不允许存在分类错误的样本，也就表示模型准确率会比较高
非线性可分SVM，重点在于低纬特征数据到高维特征数据之间的映射
	核函数，是从低维特征空间到高维特征空间的一个映射，如果存在K（x，z），对任意的低维特征向量x，z都有
		K(x,z)=$(x)*$(z)
		在低维空间上的计算量等价于特征做维度扩展后的点乘的结果
		解决线性不可分的时候，使用低维特征空间的计算量来避免高维空间中向量内积的恐怖计算量
		用低维空间中少的内积计算量来让模型具有高维空间中的线性可分的优点
	类型
		线性核函数        K(x,z)=x*z
		多项式核函数	  K(x,z)=(r x*z + b)**d
		高斯核函数		  K(x,z)=e ** -r||x-z||**2
		Sigmoid核函数     K(x,z)=tanh(r x*z + b)
		核函数可以自定义，必须是正定核函数，即Gram矩阵是半正定矩阵
SMO，序列最小优化算法，是一种解决SVM训练过程中所产生的优化问题的算法

OneClassSVM是一种异常点检测算法
